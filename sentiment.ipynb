{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.8, max_features=5000, min_df=2,\n",
      "                                 ngram_range=(1, 3))),\n",
      "                ('feature_selection',\n",
      "                 SelectKBest(k='all',\n",
      "                             score_func=<function chi2 at 0x000001C68E1FFE20>)),\n",
      "                ('model', LinearSVC(C=1))])\n",
      "Best Parameters: {'model__C': 1, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 3)}\n",
      "Accuracy: 0.9845758354755784\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "import emoji\n",
    "\n",
    "# Load the dataset\n",
    "dataframe1 = pd.read_csv(\"depress_text.csv\", index_col=0)\n",
    "dataframe2 = pd.read_csv(\"non_depress_text.csv\", index_col=0)\n",
    "\n",
    "# Gabungkan kedua DataFrame\n",
    "data = pd.concat([dataframe1, dataframe2])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data = data.reset_index()\n",
    "\n",
    "# Data Cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove double quotation marks\n",
    "    text = text.replace('\"', '')\n",
    "    # Remove RT tags\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # Remove user tags (@Usertag)\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    # Remove hashtags and keep only the text\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove emojis\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove word repetition\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "    # Handle contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords, perform stemming and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens if word not in stop_words]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "data['Cleaned_Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "# Separate the features (text) and labels (sentiment)\n",
    "X = data['Cleaned_Text']\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Pipeline for Data Processing and Modeling\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3), min_df=2, max_df=0.8, max_features=5000)),\n",
    "    ('feature_selection', SelectKBest(score_func=chi2, k='all')),\n",
    "    ('model', LinearSVC())\n",
    "])\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__max_features': [1000, 5000, 10000],\n",
    "    'model__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, error_score='raise')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model and Parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Depressed\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_text' with the actual text to predict\n",
    "text_to_predict = 'I am very depress.'\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text_to_predict)\n",
    "\n",
    "# Vectorize the cleaned text\n",
    "text_vectorized = best_model.named_steps['tfidf'].transform([cleaned_text])\n",
    "\n",
    "# Make the prediction\n",
    "prediction = best_model.named_steps['model'].predict(text_vectorized)\n",
    "\n",
    "# Map the prediction to the corresponding label\n",
    "if prediction[0] == 0:\n",
    "    result = \"Not Depressed\"\n",
    "else:\n",
    "    result = \"Depressed\"\n",
    "\n",
    "# Print the prediction result\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       184\n",
      "           1       0.98      1.00      0.99       205\n",
      "\n",
      "    accuracy                           0.98       389\n",
      "   macro avg       0.99      0.98      0.98       389\n",
      "weighted avg       0.98      0.98      0.98       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\ogs\\onedrive\\desktop\\sentiment_analysis\\.venv\\lib\\site-packages (75.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install setuptools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ogs\\onedrive\\desktop\\sentiment_analysis\\.venv\\lib\\site-packages (24.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ogs\\onedrive\\desktop\\sentiment_analysis\\.venv\\lib\\site-packages (75.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: c:\\Users\\OGS\\AppData\\Local\\Temp\\tmpghfknw1_\n",
      "Requirement already satisfied: pip in c:\\users\\ogs\\onedrive\\desktop\\sentiment_analysis\\.venv\\lib\\site-packages (24.3.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m ensurepip --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report, accuracy_score\n",
      "File \u001b[1;32mc:\\Users\\OGS\\OneDrive\\Desktop\\sentiment_analysis\\.venv\\Lib\\site-packages\\seaborn\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m _orig_rc_params \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Import seaborn objects\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\OGS\\OneDrive\\Desktop\\sentiment_analysis\\.venv\\Lib\\site-packages\\seaborn\\rcmod.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def plot_confusion(cm, accuracy):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sn.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f')\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"True value\")\n",
    "    plt.title(f\"Confusion Matrix\\nAccuracy: {accuracy*100:.2f}%\")\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix with accuracy\n",
    "plot_confusion(confusion_mat, accuracy)\n",
    "\n",
    "# Display the classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 0.9811320754716981\n",
      "True Negatives (TN): 1.0\n",
      "False Positives (FP): 1.0\n",
      "False Negatives (FN): 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate the classification report\n",
    "classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the classification report dictionary\n",
    "TP = classification_rep['1']['precision']\n",
    "TN = classification_rep['0']['precision']\n",
    "FP = classification_rep['1']['recall']\n",
    "FN = classification_rep['0']['recall']\n",
    "\n",
    "print(\"True Positives (TP):\", TP)\n",
    "print(\"True Negatives (TN):\", TN)\n",
    "print(\"False Positives (FP):\", FP)\n",
    "print(\"False Negatives (FN):\", FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlighted text:\n",
      "bla\n"
     ]
    }
   ],
   "source": [
    "# Function to identify words or tokens indicating depressive symptoms\n",
    "def identify_depressive_words(text):\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Vectorize the cleaned text\n",
    "    text_vectorized = best_model.named_steps['tfidf'].transform([cleaned_text])\n",
    "\n",
    "    # Access the feature names from the TfidfVectorizer vocabulary\n",
    "    feature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "    # Access the coefficients from the best_model\n",
    "    coefficients = best_model.named_steps['model'].coef_\n",
    "\n",
    "    # Create a dictionary to store the coefficients and their corresponding feature names\n",
    "    coefficients_dict = {feature_names[i]: coefficients[0, i] for i in range(len(feature_names))}\n",
    "\n",
    "    # Sort the coefficients dictionary based on their values\n",
    "    sorted_coefficients = sorted(coefficients_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract the top N words or tokens indicating depressive symptoms\n",
    "    top_n = 10  # Adjust the value of N as needed\n",
    "    top_n_words = [word for word, coef in sorted_coefficients[:top_n]]\n",
    "\n",
    "    # Highlight the depressive words in the cleaned text\n",
    "    highlighted_text = cleaned_text\n",
    "    for word in top_n_words:\n",
    "        highlighted_text = highlighted_text.replace(word, f\"**{word.upper()}**\")\n",
    "\n",
    "    return highlighted_text\n",
    "\n",
    "# Replace 'your_text' with the actual text to predict\n",
    "text_to_predict = input(\"Enter the text to predict: \")\n",
    "\n",
    "# Identify depressive words in the user's input and highlight them\n",
    "highlighted_text = identify_depressive_words(text_to_predict)\n",
    "\n",
    "print(\"Highlighted text:\")\n",
    "print(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words indicating depressive symptoms:\n",
      "bla\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_text' with the actual text to predict\n",
    "text_to_predict = input(\"Enter the text to predict: \")\n",
    "\n",
    "# Identify depressive words in the user's input\n",
    "depressive_words = identify_depressive_words(text_to_predict)\n",
    "\n",
    "if depressive_words:\n",
    "    print(\"Words indicating depressive symptoms:\")\n",
    "    print(depressive_words)\n",
    "else:\n",
    "    print(\"No words indicating depressive symptoms.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
